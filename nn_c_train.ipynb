{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 神经网络模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mt_c2_s0.1_b16_lr0.001_d0.5_e70\n",
      "models/model_mt_c2_s0.1_b16_lr0.001_d0.5_e70.pth\n",
      "True\n",
      "1\n",
      "GeForce MX150\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Para:\n",
    "    # tensor_board_log_dir = 'runs/exp0'\n",
    "    feature_column_start_name = 'ep_ratio_ttm'\n",
    "    feature_column_end_name = 'BR'\n",
    "\n",
    "    # 模型设置\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    classification = 2 # 2, 3\n",
    "\n",
    "    # 权重\n",
    "    cross_weight = list()\n",
    "    if classification == 3:\n",
    "        cross_weight = [1.0, 1.0 ,1.0]\n",
    "    elif classification == 2:\n",
    "        cross_weight = [1.0, 1.0]\n",
    "    elif classification == 5:\n",
    "        cross_weight = [1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "\n",
    "    batch_size = 16\n",
    "    lr = 1e-3\n",
    "    drop = 0.5\n",
    "    epochs = 70\n",
    "\n",
    "    # 数据集设置\n",
    "    month_in_sample = range(0, 1)\n",
    "    # month_test = range(36, 48)\n",
    "\n",
    "    percent_cv = 0.1 # 10% cross validation\n",
    "\n",
    "    data_path = 'data/mt_space_1d_rate_20d_17-21_pre'\n",
    "\n",
    "\n",
    "    seed = 2022\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    info_str0 = data_path[5:7]+'_'+'c'+str(classification)+'_s'+str(percent_cv)\n",
    "    info_str1 = '_b'+str(batch_size)+'_lr'+str(lr)+'_d'+str(drop)+'_e'+str(epochs)\n",
    "    info_str = info_str0 + info_str1\n",
    "\n",
    "    save_model_path = 'models/'+'model_'+info_str+'.pth'\n",
    "\n",
    "para = Para()\n",
    "print(para.info_str)\n",
    "print(para.save_model_path)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 构建训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      return_bin order_book_id board_type      sector_code  month        date  \\\n0            1.0   600519.XSHG  MainBoard  ConsumerStaples      0  2017-01-03   \n1            1.0   600519.XSHG  MainBoard  ConsumerStaples      1  2017-01-04   \n2            1.0   600519.XSHG  MainBoard  ConsumerStaples      2  2017-01-05   \n3            1.0   600519.XSHG  MainBoard  ConsumerStaples      3  2017-01-06   \n4            1.0   600519.XSHG  MainBoard  ConsumerStaples      4  2017-01-09   \n...          ...           ...        ...              ...    ...         ...   \n1189         0.0   600519.XSHG  MainBoard  ConsumerStaples   1192  2021-11-29   \n1190         0.0   600519.XSHG  MainBoard  ConsumerStaples   1193  2021-11-30   \n1191         0.0   600519.XSHG  MainBoard  ConsumerStaples   1194  2021-12-01   \n1192         0.0   600519.XSHG  MainBoard  ConsumerStaples   1195  2021-12-02   \n1193         0.0   600519.XSHG  MainBoard  ConsumerStaples   1196  2021-12-03   \n\n      yield_rate  ep_ratio_ttm  pb_ratio_ttm  sp_ratio_ttm  ...     RSI10  \\\n0      -0.092587      1.582679     -1.683954      1.603209  ...  0.401807   \n1      -0.645762      1.302261     -1.583147      1.336612  ...  1.080383   \n2      -0.369527      1.382886     -1.613186      1.413263  ...  0.609067   \n3      -0.581350      1.319990     -1.589829      1.353466  ...  0.988294   \n4      -0.357646      1.355014     -1.602902      1.386764  ...  1.230286   \n...          ...           ...           ...           ...  ...       ...   \n1189    0.439775     -1.199925      0.939052     -1.223484  ...  1.741930   \n1190    0.818726     -1.118328      0.814286     -1.150999  ...  0.771590   \n1191    0.234917     -1.121759      0.819394     -1.154046  ...  0.957610   \n1192    0.427783     -1.123316      0.821717     -1.155430  ...  1.004908   \n1193    0.050756     -1.180596      0.908867     -1.206313  ...  1.011763   \n\n            SY    BIAS20     VOL30     VOL60    VOL120    VOLT20    VOLT60  \\\n0     0.988619  0.061485 -0.671471 -1.318601 -1.576493 -1.136058 -1.217233   \n1     0.988619  1.075444 -0.532821 -1.266669 -1.548342 -1.060905 -1.199013   \n2     0.988619  0.685052 -0.505367 -1.217169 -1.570874 -1.030676 -1.189542   \n3     1.719976  0.894761 -0.401765 -1.102462 -1.549486 -0.995422 -1.178730   \n4     0.988619  0.717882 -0.355223 -1.052310 -1.575254 -0.972552 -1.170599   \n...        ...       ...       ...       ...       ...       ...       ...   \n1189  0.988619  1.417920 -0.941589 -0.287143  0.235562  1.221078  0.919459   \n1190  0.988619  0.726018 -0.953039 -0.285410  0.253470  1.322049  0.900321   \n1191  1.719976  0.651600 -0.961177 -0.338331  0.246088  1.344969  0.868925   \n1192  1.719976  0.586634 -0.969949 -0.386910  0.238948  1.402479  0.808674   \n1193  1.719976  0.921982 -0.970436 -0.473598  0.222814  1.538114  0.807235   \n\n            AR        BR  \n0     0.560313  0.139982  \n1     1.141963  0.551387  \n2     0.668555  0.023353  \n3     1.176819  0.547457  \n4     1.197702  0.310576  \n...        ...       ...  \n1189  0.760456  0.329757  \n1190  0.468537  0.267050  \n1191  0.363120  0.467004  \n1192  0.583062  0.834642  \n1193  0.987061  1.090426  \n\n[1194 rows x 23 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>return_bin</th>\n      <th>order_book_id</th>\n      <th>board_type</th>\n      <th>sector_code</th>\n      <th>month</th>\n      <th>date</th>\n      <th>yield_rate</th>\n      <th>ep_ratio_ttm</th>\n      <th>pb_ratio_ttm</th>\n      <th>sp_ratio_ttm</th>\n      <th>...</th>\n      <th>RSI10</th>\n      <th>SY</th>\n      <th>BIAS20</th>\n      <th>VOL30</th>\n      <th>VOL60</th>\n      <th>VOL120</th>\n      <th>VOLT20</th>\n      <th>VOLT60</th>\n      <th>AR</th>\n      <th>BR</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>600519.XSHG</td>\n      <td>MainBoard</td>\n      <td>ConsumerStaples</td>\n      <td>0</td>\n      <td>2017-01-03</td>\n      <td>-0.092587</td>\n      <td>1.582679</td>\n      <td>-1.683954</td>\n      <td>1.603209</td>\n      <td>...</td>\n      <td>0.401807</td>\n      <td>0.988619</td>\n      <td>0.061485</td>\n      <td>-0.671471</td>\n      <td>-1.318601</td>\n      <td>-1.576493</td>\n      <td>-1.136058</td>\n      <td>-1.217233</td>\n      <td>0.560313</td>\n      <td>0.139982</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>600519.XSHG</td>\n      <td>MainBoard</td>\n      <td>ConsumerStaples</td>\n      <td>1</td>\n      <td>2017-01-04</td>\n      <td>-0.645762</td>\n      <td>1.302261</td>\n      <td>-1.583147</td>\n      <td>1.336612</td>\n      <td>...</td>\n      <td>1.080383</td>\n      <td>0.988619</td>\n      <td>1.075444</td>\n      <td>-0.532821</td>\n      <td>-1.266669</td>\n      <td>-1.548342</td>\n      <td>-1.060905</td>\n      <td>-1.199013</td>\n      <td>1.141963</td>\n      <td>0.551387</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>600519.XSHG</td>\n      <td>MainBoard</td>\n      <td>ConsumerStaples</td>\n      <td>2</td>\n      <td>2017-01-05</td>\n      <td>-0.369527</td>\n      <td>1.382886</td>\n      <td>-1.613186</td>\n      <td>1.413263</td>\n      <td>...</td>\n      <td>0.609067</td>\n      <td>0.988619</td>\n      <td>0.685052</td>\n      <td>-0.505367</td>\n      <td>-1.217169</td>\n      <td>-1.570874</td>\n      <td>-1.030676</td>\n      <td>-1.189542</td>\n      <td>0.668555</td>\n      <td>0.023353</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>600519.XSHG</td>\n      <td>MainBoard</td>\n      <td>ConsumerStaples</td>\n      <td>3</td>\n      <td>2017-01-06</td>\n      <td>-0.581350</td>\n      <td>1.319990</td>\n      <td>-1.589829</td>\n      <td>1.353466</td>\n      <td>...</td>\n      <td>0.988294</td>\n      <td>1.719976</td>\n      <td>0.894761</td>\n      <td>-0.401765</td>\n      <td>-1.102462</td>\n      <td>-1.549486</td>\n      <td>-0.995422</td>\n      <td>-1.178730</td>\n      <td>1.176819</td>\n      <td>0.547457</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>600519.XSHG</td>\n      <td>MainBoard</td>\n      <td>ConsumerStaples</td>\n      <td>4</td>\n      <td>2017-01-09</td>\n      <td>-0.357646</td>\n      <td>1.355014</td>\n      <td>-1.602902</td>\n      <td>1.386764</td>\n      <td>...</td>\n      <td>1.230286</td>\n      <td>0.988619</td>\n      <td>0.717882</td>\n      <td>-0.355223</td>\n      <td>-1.052310</td>\n      <td>-1.575254</td>\n      <td>-0.972552</td>\n      <td>-1.170599</td>\n      <td>1.197702</td>\n      <td>0.310576</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1189</th>\n      <td>0.0</td>\n      <td>600519.XSHG</td>\n      <td>MainBoard</td>\n      <td>ConsumerStaples</td>\n      <td>1192</td>\n      <td>2021-11-29</td>\n      <td>0.439775</td>\n      <td>-1.199925</td>\n      <td>0.939052</td>\n      <td>-1.223484</td>\n      <td>...</td>\n      <td>1.741930</td>\n      <td>0.988619</td>\n      <td>1.417920</td>\n      <td>-0.941589</td>\n      <td>-0.287143</td>\n      <td>0.235562</td>\n      <td>1.221078</td>\n      <td>0.919459</td>\n      <td>0.760456</td>\n      <td>0.329757</td>\n    </tr>\n    <tr>\n      <th>1190</th>\n      <td>0.0</td>\n      <td>600519.XSHG</td>\n      <td>MainBoard</td>\n      <td>ConsumerStaples</td>\n      <td>1193</td>\n      <td>2021-11-30</td>\n      <td>0.818726</td>\n      <td>-1.118328</td>\n      <td>0.814286</td>\n      <td>-1.150999</td>\n      <td>...</td>\n      <td>0.771590</td>\n      <td>0.988619</td>\n      <td>0.726018</td>\n      <td>-0.953039</td>\n      <td>-0.285410</td>\n      <td>0.253470</td>\n      <td>1.322049</td>\n      <td>0.900321</td>\n      <td>0.468537</td>\n      <td>0.267050</td>\n    </tr>\n    <tr>\n      <th>1191</th>\n      <td>0.0</td>\n      <td>600519.XSHG</td>\n      <td>MainBoard</td>\n      <td>ConsumerStaples</td>\n      <td>1194</td>\n      <td>2021-12-01</td>\n      <td>0.234917</td>\n      <td>-1.121759</td>\n      <td>0.819394</td>\n      <td>-1.154046</td>\n      <td>...</td>\n      <td>0.957610</td>\n      <td>1.719976</td>\n      <td>0.651600</td>\n      <td>-0.961177</td>\n      <td>-0.338331</td>\n      <td>0.246088</td>\n      <td>1.344969</td>\n      <td>0.868925</td>\n      <td>0.363120</td>\n      <td>0.467004</td>\n    </tr>\n    <tr>\n      <th>1192</th>\n      <td>0.0</td>\n      <td>600519.XSHG</td>\n      <td>MainBoard</td>\n      <td>ConsumerStaples</td>\n      <td>1195</td>\n      <td>2021-12-02</td>\n      <td>0.427783</td>\n      <td>-1.123316</td>\n      <td>0.821717</td>\n      <td>-1.155430</td>\n      <td>...</td>\n      <td>1.004908</td>\n      <td>1.719976</td>\n      <td>0.586634</td>\n      <td>-0.969949</td>\n      <td>-0.386910</td>\n      <td>0.238948</td>\n      <td>1.402479</td>\n      <td>0.808674</td>\n      <td>0.583062</td>\n      <td>0.834642</td>\n    </tr>\n    <tr>\n      <th>1193</th>\n      <td>0.0</td>\n      <td>600519.XSHG</td>\n      <td>MainBoard</td>\n      <td>ConsumerStaples</td>\n      <td>1196</td>\n      <td>2021-12-03</td>\n      <td>0.050756</td>\n      <td>-1.180596</td>\n      <td>0.908867</td>\n      <td>-1.206313</td>\n      <td>...</td>\n      <td>1.011763</td>\n      <td>1.719976</td>\n      <td>0.921982</td>\n      <td>-0.970436</td>\n      <td>-0.473598</td>\n      <td>0.222814</td>\n      <td>1.538114</td>\n      <td>0.807235</td>\n      <td>0.987061</td>\n      <td>1.090426</td>\n    </tr>\n  </tbody>\n</table>\n<p>1194 rows × 23 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data_in_sample = None\n",
    "for i_month in para.month_in_sample:\n",
    "    file_name = para.data_path + '/' + str(i_month) + '.csv'\n",
    "    data_curr_month = pd.read_csv(file_name)\n",
    "\n",
    "    data_curr_month = data_curr_month.dropna(axis=0)\n",
    "\n",
    "    data_curr_month.insert(loc=0, column='return_bin', value=np.nan)\n",
    "\n",
    "    data_curr_month.loc[data_curr_month['yield_rate']>0, 'return_bin'] = 0\n",
    "    data_curr_month.loc[data_curr_month['yield_rate']<=0, 'return_bin'] = 1\n",
    "\n",
    "    if i_month == para.month_in_sample[0]:\n",
    "        data_in_sample = data_curr_month\n",
    "    else:\n",
    "        data_in_sample = pd.concat([data_in_sample, data_curr_month])\n",
    "        # data_in_sample = data_in_sample.append(data_curr_month)\n",
    "\n",
    "data_in_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      ep_ratio_ttm  pb_ratio_ttm  sp_ratio_ttm  MACD_DIFF  MACD_DEA  \\\n0         1.582679     -1.683954      1.603209  -0.237438 -0.255987   \n1         1.302261     -1.583147      1.336612  -0.175133 -0.241780   \n2         1.382886     -1.613186      1.413263  -0.143321 -0.223657   \n3         1.319990     -1.589829      1.353466  -0.108782 -0.201822   \n4         1.355014     -1.602902      1.386764  -0.090909 -0.180558   \n...            ...           ...           ...        ...       ...   \n1069     -1.643253      1.898419     -1.601503   1.665881  0.969816   \n1070     -1.643497      1.898966     -1.601721   1.682552  1.133453   \n1071     -1.675508      1.971787     -1.630363   1.758577  1.280512   \n1072     -1.696369      2.020284     -1.649028   1.851527  1.417902   \n1073     -1.608422      1.821320     -1.570337   1.664571  1.488103   \n\n      MACD_HIST     RSI10        SY    BIAS20     VOL30     VOL60    VOL120  \\\n0     -0.006208  0.401807  0.988619  0.061485 -0.671471 -1.318601 -1.576493   \n1      0.156462  1.080383  0.988619  1.075444 -0.532821 -1.266669 -1.548342   \n2      0.205494  0.609067  0.988619  0.685052 -0.505367 -1.217169 -1.570874   \n3      0.251971  0.988294  1.719976  0.894761 -0.401765 -1.102462 -1.549486   \n4      0.244829  1.230286  0.988619  0.717882 -0.355223 -1.052310 -1.575254   \n...         ...       ...       ...       ...       ...       ...       ...   \n1069   2.484405  1.200382  0.257261  1.125931 -0.499829 -0.321938  0.153455   \n1070   2.027578  1.162939  0.988619  0.987958 -0.527879 -0.406184  0.145273   \n1071   1.819983  1.507551  0.988619  1.094369 -0.517299 -0.472357  0.103294   \n1072   1.698920  1.480561  0.988619  1.080470 -0.519608 -0.591010  0.058887   \n1073   0.857595 -0.380736  0.257261  0.181024 -0.450483 -0.599069  0.039781   \n\n        VOLT20    VOLT60        AR        BR  \n0    -1.136058 -1.217233  0.560313  0.139982  \n1    -1.060905 -1.199013  1.141963  0.551387  \n2    -1.030676 -1.189542  0.668555  0.023353  \n3    -0.995422 -1.178730  1.176819  0.547457  \n4    -0.972552 -1.170599  1.197702  0.310576  \n...        ...       ...       ...       ...  \n1069  2.960621  0.339754  0.337491  0.336147  \n1070  2.981936  0.403345  0.195472  0.201351  \n1071  2.856370  0.465268  0.904794  0.478919  \n1072  2.576115  0.524389  0.909624  0.522575  \n1073  2.335896  0.537712  0.230335  0.458322  \n\n[1074 rows x 16 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ep_ratio_ttm</th>\n      <th>pb_ratio_ttm</th>\n      <th>sp_ratio_ttm</th>\n      <th>MACD_DIFF</th>\n      <th>MACD_DEA</th>\n      <th>MACD_HIST</th>\n      <th>RSI10</th>\n      <th>SY</th>\n      <th>BIAS20</th>\n      <th>VOL30</th>\n      <th>VOL60</th>\n      <th>VOL120</th>\n      <th>VOLT20</th>\n      <th>VOLT60</th>\n      <th>AR</th>\n      <th>BR</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.582679</td>\n      <td>-1.683954</td>\n      <td>1.603209</td>\n      <td>-0.237438</td>\n      <td>-0.255987</td>\n      <td>-0.006208</td>\n      <td>0.401807</td>\n      <td>0.988619</td>\n      <td>0.061485</td>\n      <td>-0.671471</td>\n      <td>-1.318601</td>\n      <td>-1.576493</td>\n      <td>-1.136058</td>\n      <td>-1.217233</td>\n      <td>0.560313</td>\n      <td>0.139982</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.302261</td>\n      <td>-1.583147</td>\n      <td>1.336612</td>\n      <td>-0.175133</td>\n      <td>-0.241780</td>\n      <td>0.156462</td>\n      <td>1.080383</td>\n      <td>0.988619</td>\n      <td>1.075444</td>\n      <td>-0.532821</td>\n      <td>-1.266669</td>\n      <td>-1.548342</td>\n      <td>-1.060905</td>\n      <td>-1.199013</td>\n      <td>1.141963</td>\n      <td>0.551387</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.382886</td>\n      <td>-1.613186</td>\n      <td>1.413263</td>\n      <td>-0.143321</td>\n      <td>-0.223657</td>\n      <td>0.205494</td>\n      <td>0.609067</td>\n      <td>0.988619</td>\n      <td>0.685052</td>\n      <td>-0.505367</td>\n      <td>-1.217169</td>\n      <td>-1.570874</td>\n      <td>-1.030676</td>\n      <td>-1.189542</td>\n      <td>0.668555</td>\n      <td>0.023353</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.319990</td>\n      <td>-1.589829</td>\n      <td>1.353466</td>\n      <td>-0.108782</td>\n      <td>-0.201822</td>\n      <td>0.251971</td>\n      <td>0.988294</td>\n      <td>1.719976</td>\n      <td>0.894761</td>\n      <td>-0.401765</td>\n      <td>-1.102462</td>\n      <td>-1.549486</td>\n      <td>-0.995422</td>\n      <td>-1.178730</td>\n      <td>1.176819</td>\n      <td>0.547457</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.355014</td>\n      <td>-1.602902</td>\n      <td>1.386764</td>\n      <td>-0.090909</td>\n      <td>-0.180558</td>\n      <td>0.244829</td>\n      <td>1.230286</td>\n      <td>0.988619</td>\n      <td>0.717882</td>\n      <td>-0.355223</td>\n      <td>-1.052310</td>\n      <td>-1.575254</td>\n      <td>-0.972552</td>\n      <td>-1.170599</td>\n      <td>1.197702</td>\n      <td>0.310576</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1069</th>\n      <td>-1.643253</td>\n      <td>1.898419</td>\n      <td>-1.601503</td>\n      <td>1.665881</td>\n      <td>0.969816</td>\n      <td>2.484405</td>\n      <td>1.200382</td>\n      <td>0.257261</td>\n      <td>1.125931</td>\n      <td>-0.499829</td>\n      <td>-0.321938</td>\n      <td>0.153455</td>\n      <td>2.960621</td>\n      <td>0.339754</td>\n      <td>0.337491</td>\n      <td>0.336147</td>\n    </tr>\n    <tr>\n      <th>1070</th>\n      <td>-1.643497</td>\n      <td>1.898966</td>\n      <td>-1.601721</td>\n      <td>1.682552</td>\n      <td>1.133453</td>\n      <td>2.027578</td>\n      <td>1.162939</td>\n      <td>0.988619</td>\n      <td>0.987958</td>\n      <td>-0.527879</td>\n      <td>-0.406184</td>\n      <td>0.145273</td>\n      <td>2.981936</td>\n      <td>0.403345</td>\n      <td>0.195472</td>\n      <td>0.201351</td>\n    </tr>\n    <tr>\n      <th>1071</th>\n      <td>-1.675508</td>\n      <td>1.971787</td>\n      <td>-1.630363</td>\n      <td>1.758577</td>\n      <td>1.280512</td>\n      <td>1.819983</td>\n      <td>1.507551</td>\n      <td>0.988619</td>\n      <td>1.094369</td>\n      <td>-0.517299</td>\n      <td>-0.472357</td>\n      <td>0.103294</td>\n      <td>2.856370</td>\n      <td>0.465268</td>\n      <td>0.904794</td>\n      <td>0.478919</td>\n    </tr>\n    <tr>\n      <th>1072</th>\n      <td>-1.696369</td>\n      <td>2.020284</td>\n      <td>-1.649028</td>\n      <td>1.851527</td>\n      <td>1.417902</td>\n      <td>1.698920</td>\n      <td>1.480561</td>\n      <td>0.988619</td>\n      <td>1.080470</td>\n      <td>-0.519608</td>\n      <td>-0.591010</td>\n      <td>0.058887</td>\n      <td>2.576115</td>\n      <td>0.524389</td>\n      <td>0.909624</td>\n      <td>0.522575</td>\n    </tr>\n    <tr>\n      <th>1073</th>\n      <td>-1.608422</td>\n      <td>1.821320</td>\n      <td>-1.570337</td>\n      <td>1.664571</td>\n      <td>1.488103</td>\n      <td>0.857595</td>\n      <td>-0.380736</td>\n      <td>0.257261</td>\n      <td>0.181024</td>\n      <td>-0.450483</td>\n      <td>-0.599069</td>\n      <td>0.039781</td>\n      <td>2.335896</td>\n      <td>0.537712</td>\n      <td>0.230335</td>\n      <td>0.458322</td>\n    </tr>\n  </tbody>\n</table>\n<p>1074 rows × 16 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_in_sample = data_in_sample.loc[:, para.feature_column_start_name: para.feature_column_end_name]\n",
    "y_in_sample = data_in_sample.loc[:, 'return_bin']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_in_sample, y_in_sample, test_size=para.percent_cv, shuffle=False) # True, random_state=para.seed)\n",
    "# X_train, X_cv, y_train, y_cv = train_test_split(X_in_sample, y_in_sample, test_size=para.percent_cv, shuffle=False)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0       1.0\n1       1.0\n2       1.0\n3       1.0\n4       1.0\n       ... \n1069    1.0\n1070    1.0\n1071    1.0\n1072    1.0\n1073    1.0\nName: return_bin, Length: 1074, dtype: float64"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      ep_ratio_ttm  pb_ratio_ttm  sp_ratio_ttm  MACD_DIFF  MACD_DEA  \\\n0         1.582679     -1.683954      1.603209  -0.237438 -0.255987   \n1         1.302261     -1.583147      1.336612  -0.175133 -0.241780   \n2         1.382886     -1.613186      1.413263  -0.143321 -0.223657   \n3         1.319990     -1.589829      1.353466  -0.108782 -0.201822   \n4         1.355014     -1.602902      1.386764  -0.090909 -0.180558   \n...            ...           ...           ...        ...       ...   \n1069     -1.643253      1.898419     -1.601503   1.665881  0.969816   \n1070     -1.643497      1.898966     -1.601721   1.682552  1.133453   \n1071     -1.675508      1.971787     -1.630363   1.758577  1.280512   \n1072     -1.696369      2.020284     -1.649028   1.851527  1.417902   \n1073     -1.608422      1.821320     -1.570337   1.664571  1.488103   \n\n      MACD_HIST     RSI10        SY    BIAS20     VOL30     VOL60    VOL120  \\\n0     -0.006208  0.401807  0.988619  0.061485 -0.671471 -1.318601 -1.576493   \n1      0.156462  1.080383  0.988619  1.075444 -0.532821 -1.266669 -1.548342   \n2      0.205494  0.609067  0.988619  0.685052 -0.505367 -1.217169 -1.570874   \n3      0.251971  0.988294  1.719976  0.894761 -0.401765 -1.102462 -1.549486   \n4      0.244829  1.230286  0.988619  0.717882 -0.355223 -1.052310 -1.575254   \n...         ...       ...       ...       ...       ...       ...       ...   \n1069   2.484405  1.200382  0.257261  1.125931 -0.499829 -0.321938  0.153455   \n1070   2.027578  1.162939  0.988619  0.987958 -0.527879 -0.406184  0.145273   \n1071   1.819983  1.507551  0.988619  1.094369 -0.517299 -0.472357  0.103294   \n1072   1.698920  1.480561  0.988619  1.080470 -0.519608 -0.591010  0.058887   \n1073   0.857595 -0.380736  0.257261  0.181024 -0.450483 -0.599069  0.039781   \n\n        VOLT20    VOLT60        AR        BR  return_bin  \n0    -1.136058 -1.217233  0.560313  0.139982         1.0  \n1    -1.060905 -1.199013  1.141963  0.551387         1.0  \n2    -1.030676 -1.189542  0.668555  0.023353         1.0  \n3    -0.995422 -1.178730  1.176819  0.547457         1.0  \n4    -0.972552 -1.170599  1.197702  0.310576         1.0  \n...        ...       ...       ...       ...         ...  \n1069  2.960621  0.339754  0.337491  0.336147         1.0  \n1070  2.981936  0.403345  0.195472  0.201351         1.0  \n1071  2.856370  0.465268  0.904794  0.478919         1.0  \n1072  2.576115  0.524389  0.909624  0.522575         1.0  \n1073  2.335896  0.537712  0.230335  0.458322         1.0  \n\n[1074 rows x 17 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ep_ratio_ttm</th>\n      <th>pb_ratio_ttm</th>\n      <th>sp_ratio_ttm</th>\n      <th>MACD_DIFF</th>\n      <th>MACD_DEA</th>\n      <th>MACD_HIST</th>\n      <th>RSI10</th>\n      <th>SY</th>\n      <th>BIAS20</th>\n      <th>VOL30</th>\n      <th>VOL60</th>\n      <th>VOL120</th>\n      <th>VOLT20</th>\n      <th>VOLT60</th>\n      <th>AR</th>\n      <th>BR</th>\n      <th>return_bin</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.582679</td>\n      <td>-1.683954</td>\n      <td>1.603209</td>\n      <td>-0.237438</td>\n      <td>-0.255987</td>\n      <td>-0.006208</td>\n      <td>0.401807</td>\n      <td>0.988619</td>\n      <td>0.061485</td>\n      <td>-0.671471</td>\n      <td>-1.318601</td>\n      <td>-1.576493</td>\n      <td>-1.136058</td>\n      <td>-1.217233</td>\n      <td>0.560313</td>\n      <td>0.139982</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.302261</td>\n      <td>-1.583147</td>\n      <td>1.336612</td>\n      <td>-0.175133</td>\n      <td>-0.241780</td>\n      <td>0.156462</td>\n      <td>1.080383</td>\n      <td>0.988619</td>\n      <td>1.075444</td>\n      <td>-0.532821</td>\n      <td>-1.266669</td>\n      <td>-1.548342</td>\n      <td>-1.060905</td>\n      <td>-1.199013</td>\n      <td>1.141963</td>\n      <td>0.551387</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.382886</td>\n      <td>-1.613186</td>\n      <td>1.413263</td>\n      <td>-0.143321</td>\n      <td>-0.223657</td>\n      <td>0.205494</td>\n      <td>0.609067</td>\n      <td>0.988619</td>\n      <td>0.685052</td>\n      <td>-0.505367</td>\n      <td>-1.217169</td>\n      <td>-1.570874</td>\n      <td>-1.030676</td>\n      <td>-1.189542</td>\n      <td>0.668555</td>\n      <td>0.023353</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.319990</td>\n      <td>-1.589829</td>\n      <td>1.353466</td>\n      <td>-0.108782</td>\n      <td>-0.201822</td>\n      <td>0.251971</td>\n      <td>0.988294</td>\n      <td>1.719976</td>\n      <td>0.894761</td>\n      <td>-0.401765</td>\n      <td>-1.102462</td>\n      <td>-1.549486</td>\n      <td>-0.995422</td>\n      <td>-1.178730</td>\n      <td>1.176819</td>\n      <td>0.547457</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.355014</td>\n      <td>-1.602902</td>\n      <td>1.386764</td>\n      <td>-0.090909</td>\n      <td>-0.180558</td>\n      <td>0.244829</td>\n      <td>1.230286</td>\n      <td>0.988619</td>\n      <td>0.717882</td>\n      <td>-0.355223</td>\n      <td>-1.052310</td>\n      <td>-1.575254</td>\n      <td>-0.972552</td>\n      <td>-1.170599</td>\n      <td>1.197702</td>\n      <td>0.310576</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1069</th>\n      <td>-1.643253</td>\n      <td>1.898419</td>\n      <td>-1.601503</td>\n      <td>1.665881</td>\n      <td>0.969816</td>\n      <td>2.484405</td>\n      <td>1.200382</td>\n      <td>0.257261</td>\n      <td>1.125931</td>\n      <td>-0.499829</td>\n      <td>-0.321938</td>\n      <td>0.153455</td>\n      <td>2.960621</td>\n      <td>0.339754</td>\n      <td>0.337491</td>\n      <td>0.336147</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1070</th>\n      <td>-1.643497</td>\n      <td>1.898966</td>\n      <td>-1.601721</td>\n      <td>1.682552</td>\n      <td>1.133453</td>\n      <td>2.027578</td>\n      <td>1.162939</td>\n      <td>0.988619</td>\n      <td>0.987958</td>\n      <td>-0.527879</td>\n      <td>-0.406184</td>\n      <td>0.145273</td>\n      <td>2.981936</td>\n      <td>0.403345</td>\n      <td>0.195472</td>\n      <td>0.201351</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1071</th>\n      <td>-1.675508</td>\n      <td>1.971787</td>\n      <td>-1.630363</td>\n      <td>1.758577</td>\n      <td>1.280512</td>\n      <td>1.819983</td>\n      <td>1.507551</td>\n      <td>0.988619</td>\n      <td>1.094369</td>\n      <td>-0.517299</td>\n      <td>-0.472357</td>\n      <td>0.103294</td>\n      <td>2.856370</td>\n      <td>0.465268</td>\n      <td>0.904794</td>\n      <td>0.478919</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1072</th>\n      <td>-1.696369</td>\n      <td>2.020284</td>\n      <td>-1.649028</td>\n      <td>1.851527</td>\n      <td>1.417902</td>\n      <td>1.698920</td>\n      <td>1.480561</td>\n      <td>0.988619</td>\n      <td>1.080470</td>\n      <td>-0.519608</td>\n      <td>-0.591010</td>\n      <td>0.058887</td>\n      <td>2.576115</td>\n      <td>0.524389</td>\n      <td>0.909624</td>\n      <td>0.522575</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1073</th>\n      <td>-1.608422</td>\n      <td>1.821320</td>\n      <td>-1.570337</td>\n      <td>1.664571</td>\n      <td>1.488103</td>\n      <td>0.857595</td>\n      <td>-0.380736</td>\n      <td>0.257261</td>\n      <td>0.181024</td>\n      <td>-0.450483</td>\n      <td>-0.599069</td>\n      <td>0.039781</td>\n      <td>2.335896</td>\n      <td>0.537712</td>\n      <td>0.230335</td>\n      <td>0.458322</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1074 rows × 17 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.concat([X_train, y_train], axis=1)\n",
    "data_cv = pd.concat([X_cv, y_cv], axis=1)\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "import torch\n",
    "\n",
    "X_train_ndarray = X_train.values\n",
    "y_train_ndarray = y_train.values\n",
    "\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train_ndarray).type(torch.FloatTensor), torch.from_numpy(y_train_ndarray).type(torch.LongTensor))\n",
    "\n",
    "# for X_train_temp, y_train_temp in train_dataset:\n",
    "#     print(X_train_temp, y_train_temp)\n",
    "#     print(X_train_temp.dtype, y_train_temp.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_cv_ndarray = X_cv.values\n",
    "y_cv_ndarray = y_cv.values\n",
    "\n",
    "cv_dataset = TensorDataset(torch.from_numpy(X_cv_ndarray).type(torch.FloatTensor), torch.from_numpy(y_cv.values).type(torch.LongTensor))\n",
    "\n",
    "# for X_cv_temp, y_cv_temp in cv_dataset:\n",
    "#     print(X_cv_temp, y_cv_temp)\n",
    "#     print(X_cv_temp.dtype, y_cv_temp.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=para.batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "cv_dataloader = DataLoader(\n",
    "    dataset=cv_dataset,\n",
    "    batch_size=para.batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 构建测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# data_test = None\n",
    "# for i_month in para.month_test:\n",
    "#\n",
    "#     file_name = para.data_path + '/' + str(i_month) + '.csv'\n",
    "#     data_curr_month = pd.read_csv(file_name)\n",
    "#\n",
    "#     data_curr_month = data_curr_month.dropna(axis=0)\n",
    "#\n",
    "#     data_curr_month = label_data(data=data_curr_month, percent_select=para.percent_select)\n",
    "#\n",
    "#     if i_month == para.month_test[0]:\n",
    "#         data_test = data_curr_month\n",
    "#     else:\n",
    "#         data_test = pd.concat([data_test, data_curr_month])\n",
    "#         # data_test = data_test.append(data_curr_month)\n",
    "#\n",
    "# X_test = data_test.loc[:, para.feature_column_start_name: para.feature_column_end_name]\n",
    "# y_test = data_test.loc[:, 'return_bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from torch.utils.data import TensorDataset\n",
    "# import torch\n",
    "#\n",
    "#\n",
    "# X_test_ndarray = X_test.values\n",
    "# y_test_ndarray = y_test.values\n",
    "#\n",
    "# test_dataset = TensorDataset(torch.from_numpy(X_test_ndarray).type(torch.FloatTensor), torch.from_numpy(y_test_ndarray).type(torch.LongTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "#\n",
    "#\n",
    "# test_dataloader = DataLoader(\n",
    "#     dataset=test_dataset,\n",
    "#     batch_size=para.batch_size,\n",
    "#     shuffle=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 构建神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (linear1): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (relu1): ReLU()\n",
      "  (linear2): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (relu2): ReLU()\n",
      "  (linear3): Linear(in_features=8, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from my_utils.model_class import MLP\n",
    "\n",
    "model = MLP(in_nums=len(X_train.columns), out_nums=para.classification, drop_p=para.drop)\n",
    "# to device\n",
    "model = model.to(device=para.device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 训练与测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    train_loss, correct = 0, 0\n",
    "\n",
    "    # initialize metric\n",
    "    train_precision = torchmetrics.Precision(average='none', num_classes=para.classification).to(device=para.device)\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        # to device\n",
    "        X = X.to(device=para.device)\n",
    "        y = y.to(device=para.device)\n",
    "\n",
    "        # compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        train_loss += loss_fn(pred, y).item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        # metric on current batch\n",
    "        train_precision(pred.argmax(1), y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if batch % 10 == 0:\n",
    "        #     loss, current = loss.item(), batch * len(X)\n",
    "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    train_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Train Error: \\n    Accuracy: {(100*correct):>0.1f}%, Avg loss: {train_loss:>8f} \\n\")\n",
    "\n",
    "    # metric on all batches using custom accumulation\n",
    "    total_precision = train_precision.compute()\n",
    "    print(\"Precision of every train dataset class: \", total_precision)\n",
    "    print()\n",
    "\n",
    "    return correct, train_loss, total_precision\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # initialize metric\n",
    "    test_precision = torchmetrics.Precision(average='none', num_classes=para.classification).to(device=para.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "\n",
    "            # to device\n",
    "            X = X.to(device=para.device)\n",
    "            y = y.to(device=para.device)\n",
    "\n",
    "            # compute prediction and loss\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "            # metric on current batch\n",
    "            test_precision(pred.argmax(1), y)\n",
    "\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n    Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    # metric on all batches using custom accumulation\n",
    "    total_precision = test_precision.compute()\n",
    "    print(\"Precision of every test dataset class: \", total_precision)\n",
    "    print()\n",
    "\n",
    "    return correct, test_loss, total_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def select_df_to_dataloader(df: pd.DataFrame, select: int) -> DataLoader:\n",
    "\n",
    "    df = df[df['return_bin'] == select]\n",
    "\n",
    "    df_dataset = TensorDataset(\n",
    "        torch.from_numpy(df.loc[:, para.feature_column_start_name: para.feature_column_end_name].values).type(torch.FloatTensor),\n",
    "        torch.from_numpy(df.loc[:, 'return_bin'].values).type(torch.LongTensor))\n",
    "\n",
    "    df_dataloader = DataLoader(\n",
    "        dataset=df_dataset,\n",
    "        batch_size=para.batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    return df_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# temp2_dataloader = select_df_to_dataloader(df=data_cv, select=2)\n",
    "temp1_dataloader = select_df_to_dataloader(df=data_cv, select=1)\n",
    "temp0_dataloader = select_df_to_dataloader(df=data_cv, select=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 53.9%, Avg loss: 0.691920 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5568, 0.5264], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 57.5%, Avg loss: 0.683194 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.5798], device='cuda:0')\n",
      "\n",
      "Time cost = 3.083879s\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 52.0%, Avg loss: 0.694395 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5311, 0.5101], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 58.3%, Avg loss: 0.684067 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.5833], device='cuda:0')\n",
      "\n",
      "Time cost = 4.248967s\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 54.1%, Avg loss: 0.683649 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5588, 0.5281], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 58.3%, Avg loss: 0.684005 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.5833], device='cuda:0')\n",
      "\n",
      "Time cost = 4.954712s\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 52.6%, Avg loss: 0.682006 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5383, 0.5161], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 58.3%, Avg loss: 0.675232 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.5833], device='cuda:0')\n",
      "\n",
      "Time cost = 5.557829s\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 54.1%, Avg loss: 0.688135 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5604, 0.5276], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 58.3%, Avg loss: 0.680016 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.5833], device='cuda:0')\n",
      "\n",
      "Time cost = 6.370633s\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 53.3%, Avg loss: 0.683388 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5505, 0.5204], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 58.3%, Avg loss: 0.676640 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.5833], device='cuda:0')\n",
      "\n",
      "Time cost = 7.601026s\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 56.4%, Avg loss: 0.673532 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5774, 0.5526], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 58.3%, Avg loss: 0.683902 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.5833], device='cuda:0')\n",
      "\n",
      "Time cost = 8.435532s\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 56.9%, Avg loss: 0.678612 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5740, 0.5634], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 58.3%, Avg loss: 0.675726 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.5833], device='cuda:0')\n",
      "\n",
      "Time cost = 9.153710s\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 59.4%, Avg loss: 0.663461 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5932, 0.5950], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 58.3%, Avg loss: 0.672493 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.5833], device='cuda:0')\n",
      "\n",
      "Time cost = 10.575876s\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 59.5%, Avg loss: 0.662516 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5917, 0.5991], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 58.3%, Avg loss: 0.671003 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.5833], device='cuda:0')\n",
      "\n",
      "Time cost = 11.787433s\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 58.3%, Avg loss: 0.673301 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5881, 0.5772], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 59.2%, Avg loss: 0.663677 \n",
      "\n",
      "Precision of every test dataset class:  tensor([1.0000, 0.5882], device='cuda:0')\n",
      "\n",
      "Time cost = 12.741432s\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 55.6%, Avg loss: 0.677908 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5592, 0.5519], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 58.3%, Avg loss: 0.657578 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.0000, 0.5833], device='cuda:0')\n",
      "\n",
      "Time cost = 13.823064s\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 60.3%, Avg loss: 0.671580 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6045, 0.6020], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 58.3%, Avg loss: 0.658490 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.5000, 0.5847], device='cuda:0')\n",
      "\n",
      "Time cost = 14.770913s\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 58.9%, Avg loss: 0.662430 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5882, 0.5908], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 60.8%, Avg loss: 0.658361 \n",
      "\n",
      "Precision of every test dataset class:  tensor([1.0000, 0.5983], device='cuda:0')\n",
      "\n",
      "Time cost = 15.526390s\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 58.5%, Avg loss: 0.669574 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5919, 0.5774], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 60.0%, Avg loss: 0.649814 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7500, 0.5948], device='cuda:0')\n",
      "\n",
      "Time cost = 16.379264s\n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 58.7%, Avg loss: 0.657704 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5934, 0.5795], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 60.0%, Avg loss: 0.651100 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7500, 0.5948], device='cuda:0')\n",
      "\n",
      "Time cost = 17.103945s\n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 61.7%, Avg loss: 0.656057 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6208, 0.6136], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 59.2%, Avg loss: 0.644680 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6667, 0.5897], device='cuda:0')\n",
      "\n",
      "Time cost = 17.957626s\n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 57.8%, Avg loss: 0.659920 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5817, 0.5743], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 58.3%, Avg loss: 0.641846 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.5000, 0.5847], device='cuda:0')\n",
      "\n",
      "Time cost = 19.300907s\n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 58.9%, Avg loss: 0.666721 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.5939, 0.5845], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 58.3%, Avg loss: 0.629469 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.5000, 0.5847], device='cuda:0')\n",
      "\n",
      "Time cost = 20.223341s\n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 60.2%, Avg loss: 0.656642 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6076, 0.5969], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 58.3%, Avg loss: 0.633477 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.5000, 0.5847], device='cuda:0')\n",
      "\n",
      "Time cost = 20.970085s\n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 60.4%, Avg loss: 0.654369 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6078, 0.6004], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 59.2%, Avg loss: 0.637945 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6667, 0.5897], device='cuda:0')\n",
      "\n",
      "Time cost = 21.820919s\n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 60.3%, Avg loss: 0.648981 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6068, 0.5996], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 59.2%, Avg loss: 0.633203 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6000, 0.5913], device='cuda:0')\n",
      "\n",
      "Time cost = 22.460743s\n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 62.0%, Avg loss: 0.638287 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6162, 0.6250], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 60.8%, Avg loss: 0.620716 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6667, 0.6036], device='cuda:0')\n",
      "\n",
      "Time cost = 23.121698s\n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 61.6%, Avg loss: 0.654817 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6147, 0.6184], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 63.3%, Avg loss: 0.615731 \n",
      "\n",
      "Precision of every test dataset class:  tensor([1.0000, 0.6140], device='cuda:0')\n",
      "\n",
      "Time cost = 23.685801s\n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 61.0%, Avg loss: 0.644197 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6116, 0.6079], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 62.5%, Avg loss: 0.610720 \n",
      "\n",
      "Precision of every test dataset class:  tensor([1.0000, 0.6087], device='cuda:0')\n",
      "\n",
      "Time cost = 24.299244s\n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 60.5%, Avg loss: 0.643109 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6041, 0.6066], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 64.2%, Avg loss: 0.610153 \n",
      "\n",
      "Precision of every test dataset class:  tensor([1.0000, 0.6195], device='cuda:0')\n",
      "\n",
      "Time cost = 24.858245s\n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 60.9%, Avg loss: 0.649007 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6068, 0.6116], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 65.0%, Avg loss: 0.615640 \n",
      "\n",
      "Precision of every test dataset class:  tensor([1.0000, 0.6250], device='cuda:0')\n",
      "\n",
      "Time cost = 25.432391s\n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 61.8%, Avg loss: 0.640662 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6241, 0.6122], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 66.7%, Avg loss: 0.622449 \n",
      "\n",
      "Precision of every test dataset class:  tensor([1.0000, 0.6364], device='cuda:0')\n",
      "\n",
      "Time cost = 26.010937s\n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 62.2%, Avg loss: 0.642332 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6232, 0.6206], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.2%, Avg loss: 0.610244 \n",
      "\n",
      "Precision of every test dataset class:  tensor([1.0000, 0.6542], device='cuda:0')\n",
      "\n",
      "Time cost = 26.643645s\n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 61.9%, Avg loss: 0.648288 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6191, 0.6192], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 70.0%, Avg loss: 0.611573 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.9375, 0.6635], device='cuda:0')\n",
      "\n",
      "Time cost = 27.213655s\n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 63.7%, Avg loss: 0.625280 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6450, 0.6287], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 61.7%, Avg loss: 0.627775 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.5625, 0.6364], device='cuda:0')\n",
      "\n",
      "Time cost = 27.825031s\n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 65.0%, Avg loss: 0.619419 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6451, 0.6557], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 64.2%, Avg loss: 0.621062 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6207, 0.6484], device='cuda:0')\n",
      "\n",
      "Time cost = 28.437887s\n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 64.5%, Avg loss: 0.627108 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6415, 0.6497], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 62.5%, Avg loss: 0.610910 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.5758, 0.6437], device='cuda:0')\n",
      "\n",
      "Time cost = 28.983974s\n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 62.0%, Avg loss: 0.666509 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6219, 0.6181], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 65.0%, Avg loss: 0.598240 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6111, 0.6667], device='cuda:0')\n",
      "\n",
      "Time cost = 29.539976s\n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 63.5%, Avg loss: 0.633548 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6356, 0.6344], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 75.0%, Avg loss: 0.576599 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.9545, 0.7041], device='cuda:0')\n",
      "\n",
      "Time cost = 30.096890s\n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 63.4%, Avg loss: 0.625909 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6364, 0.6316], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 76.7%, Avg loss: 0.580052 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.9583, 0.7188], device='cuda:0')\n",
      "\n",
      "Time cost = 30.712892s\n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 64.9%, Avg loss: 0.617272 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6444, 0.6544], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 65.8%, Avg loss: 0.596089 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6000, 0.6933], device='cuda:0')\n",
      "\n",
      "Time cost = 31.361566s\n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 64.2%, Avg loss: 0.620160 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6435, 0.6394], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 74.2%, Avg loss: 0.579986 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.9130, 0.7010], device='cuda:0')\n",
      "\n",
      "Time cost = 32.139068s\n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 64.0%, Avg loss: 0.615667 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6422, 0.6369], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 68.3%, Avg loss: 0.587181 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6875, 0.6818], device='cuda:0')\n",
      "\n",
      "Time cost = 33.210098s\n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 64.3%, Avg loss: 0.609858 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6403, 0.6471], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 65.0%, Avg loss: 0.615242 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6000, 0.6750], device='cuda:0')\n",
      "\n",
      "Time cost = 34.032024s\n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 63.7%, Avg loss: 0.613550 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6383, 0.6353], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 74.2%, Avg loss: 0.559883 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.9130, 0.7010], device='cuda:0')\n",
      "\n",
      "Time cost = 34.659781s\n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 63.5%, Avg loss: 0.625314 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6346, 0.6355], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 73.3%, Avg loss: 0.565115 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.8462, 0.7021], device='cuda:0')\n",
      "\n",
      "Time cost = 35.385884s\n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 65.7%, Avg loss: 0.613231 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6606, 0.6538], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.2%, Avg loss: 0.568256 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6857, 0.6941], device='cuda:0')\n",
      "\n",
      "Time cost = 36.047817s\n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 67.9%, Avg loss: 0.595289 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6865, 0.6710], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 70.8%, Avg loss: 0.559551 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7419, 0.6966], device='cuda:0')\n",
      "\n",
      "Time cost = 36.727507s\n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 64.6%, Avg loss: 0.607967 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6412, 0.6523], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 69.2%, Avg loss: 0.573815 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7241, 0.6813], device='cuda:0')\n",
      "\n",
      "Time cost = 37.424044s\n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 68.1%, Avg loss: 0.597836 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6883, 0.6729], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 67.5%, Avg loss: 0.560531 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6571, 0.6824], device='cuda:0')\n",
      "\n",
      "Time cost = 38.088963s\n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 64.3%, Avg loss: 0.614255 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6485, 0.6381], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 67.5%, Avg loss: 0.570854 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6571, 0.6824], device='cuda:0')\n",
      "\n",
      "Time cost = 38.657954s\n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 65.8%, Avg loss: 0.618332 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6642, 0.6522], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 70.0%, Avg loss: 0.561020 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6750, 0.7125], device='cuda:0')\n",
      "\n",
      "Time cost = 39.251986s\n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 67.6%, Avg loss: 0.605612 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6854, 0.6667], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 71.7%, Avg loss: 0.553261 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6739, 0.7432], device='cuda:0')\n",
      "\n",
      "Time cost = 39.819039s\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 67.1%, Avg loss: 0.591714 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6655, 0.6782], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 78.3%, Avg loss: 0.523657 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.8158, 0.7683], device='cuda:0')\n",
      "\n",
      "Time cost = 40.380913s\n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 66.3%, Avg loss: 0.595281 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6631, 0.6627], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 74.2%, Avg loss: 0.528740 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7111, 0.7600], device='cuda:0')\n",
      "\n",
      "Time cost = 40.995995s\n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 67.3%, Avg loss: 0.599472 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6714, 0.6752], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 67.5%, Avg loss: 0.545307 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6000, 0.7385], device='cuda:0')\n",
      "\n",
      "Time cost = 41.717242s\n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 70.0%, Avg loss: 0.591795 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6979, 0.7028], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 72.5%, Avg loss: 0.544975 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6735, 0.7606], device='cuda:0')\n",
      "\n",
      "Time cost = 42.335916s\n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 67.2%, Avg loss: 0.590866 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6678, 0.6774], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 79.2%, Avg loss: 0.523695 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7907, 0.7922], device='cuda:0')\n",
      "\n",
      "Time cost = 43.372012s\n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 65.5%, Avg loss: 0.583827 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6699, 0.6421], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 68.3%, Avg loss: 0.552224 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6071, 0.7500], device='cuda:0')\n",
      "\n",
      "Time cost = 43.976061s\n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 66.1%, Avg loss: 0.591093 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6585, 0.6640], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 70.8%, Avg loss: 0.538629 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6415, 0.7612], device='cuda:0')\n",
      "\n",
      "Time cost = 44.794709s\n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 68.2%, Avg loss: 0.587301 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6815, 0.6816], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 77.5%, Avg loss: 0.510526 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7674, 0.7792], device='cuda:0')\n",
      "\n",
      "Time cost = 45.749838s\n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 69.6%, Avg loss: 0.586758 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.7118, 0.6804], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 73.3%, Avg loss: 0.519666 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6875, 0.7639], device='cuda:0')\n",
      "\n",
      "Time cost = 46.520924s\n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 70.0%, Avg loss: 0.574694 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.7171, 0.6846], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 71.7%, Avg loss: 0.520540 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6538, 0.7647], device='cuda:0')\n",
      "\n",
      "Time cost = 47.107513s\n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 68.7%, Avg loss: 0.585836 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6909, 0.6832], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 71.7%, Avg loss: 0.551138 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6250, 0.8214], device='cuda:0')\n",
      "\n",
      "Time cost = 47.722869s\n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 67.8%, Avg loss: 0.575081 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6724, 0.6842], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 75.0%, Avg loss: 0.514485 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7000, 0.7857], device='cuda:0')\n",
      "\n",
      "Time cost = 48.293730s\n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 68.6%, Avg loss: 0.573821 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6975, 0.6752], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 79.2%, Avg loss: 0.491231 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7119, 0.8689], device='cuda:0')\n",
      "\n",
      "Time cost = 48.864949s\n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 69.7%, Avg loss: 0.578355 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.7013, 0.6933], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 80.0%, Avg loss: 0.494739 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7500, 0.8382], device='cuda:0')\n",
      "\n",
      "Time cost = 49.422302s\n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 70.3%, Avg loss: 0.566817 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.7129, 0.6932], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 75.8%, Avg loss: 0.530513 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6780, 0.8361], device='cuda:0')\n",
      "\n",
      "Time cost = 50.047546s\n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 68.9%, Avg loss: 0.575242 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6963, 0.6816], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 81.7%, Avg loss: 0.481666 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.8889, 0.7857], device='cuda:0')\n",
      "\n",
      "Time cost = 50.636797s\n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 67.6%, Avg loss: 0.580574 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6964, 0.6579], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 77.5%, Avg loss: 0.486803 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7674, 0.7792], device='cuda:0')\n",
      "\n",
      "Time cost = 51.372896s\n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 70.9%, Avg loss: 0.581128 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.7216, 0.6978], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 77.5%, Avg loss: 0.490639 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7170, 0.8209], device='cuda:0')\n",
      "\n",
      "Time cost = 52.504637s\n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 69.2%, Avg loss: 0.579983 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.7032, 0.6807], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 73.3%, Avg loss: 0.515658 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.6452, 0.8276], device='cuda:0')\n",
      "\n",
      "Time cost = 54.019131s\n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 70.0%, Avg loss: 0.566414 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.7029, 0.6973], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 81.7%, Avg loss: 0.496427 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.7593, 0.8636], device='cuda:0')\n",
      "\n",
      "Time cost = 54.675880s\n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "Train Error: \n",
      "    Accuracy: 68.9%, Avg loss: 0.576075 \n",
      "\n",
      "Precision of every train dataset class:  tensor([0.6941, 0.6837], device='cuda:0')\n",
      "\n",
      "Test Error: \n",
      "    Accuracy: 84.2%, Avg loss: 0.470322 \n",
      "\n",
      "Precision of every test dataset class:  tensor([0.8039, 0.8696], device='cuda:0')\n",
      "\n",
      "Time cost = 55.690509s\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# 计时\n",
    "time_start = time.time()\n",
    "\n",
    "# writer = SummaryWriter(para.tensor_board_log_dir)\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# 损失函数\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# to device\n",
    "loss_fn = loss_fn.to(device=para.device)\n",
    "\n",
    "# 优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=para.lr)\n",
    "\n",
    "epochs = para.epochs\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "\n",
    "    model.train()\n",
    "    accuracy_train, loss_train, precision_train = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    model.eval()\n",
    "    accuracy_cv, loss_cv, precision_cv = test_loop(cv_dataloader, model, loss_fn)\n",
    "\n",
    "    # accuracy2 = test_loop(temp2_dataloader, model, loss_fn)\n",
    "    # print('#')\n",
    "    # accuracy1 = test_loop(temp1_dataloader, model, loss_fn)\n",
    "    # accuracy0 = test_loop(temp0_dataloader, model, loss_fn)\n",
    "\n",
    "    # 写入 tensorboard\n",
    "    if para.classification == 2:\n",
    "\n",
    "        writer.add_scalars(main_tag=para.info_str+'_evaluation/cv',\n",
    "                           tag_scalar_dict={\n",
    "                               'accuracy': accuracy_cv,\n",
    "                               'precision0': precision_cv[0],\n",
    "                               'precision1': precision_cv[1]},\n",
    "                           global_step=t)\n",
    "\n",
    "        writer.add_scalars(main_tag=para.info_str+'_evaluation/train',\n",
    "                           tag_scalar_dict={\n",
    "                               'accuracy': accuracy_train,\n",
    "                               'precision0': precision_train[0],\n",
    "                               'precision1': precision_train[1]},\n",
    "                           global_step=t)\n",
    "\n",
    "    elif para.classification == 3:\n",
    "\n",
    "        writer.add_scalars(main_tag=para.info_str+'_evaluation/cv',\n",
    "                           tag_scalar_dict={\n",
    "                               'accuracy': accuracy_cv,\n",
    "                               'precision0': precision_cv[0],\n",
    "                               'precision1': precision_cv[1],\n",
    "                               'precision2': precision_cv[2]},\n",
    "                           global_step=t)\n",
    "\n",
    "        writer.add_scalars(main_tag=para.info_str+'_evaluation/train',\n",
    "                           tag_scalar_dict={\n",
    "                               'accuracy': accuracy_train,\n",
    "                               'precision0': precision_train[0],\n",
    "                               'precision1': precision_train[1],\n",
    "                               'precision2': precision_train[2]},\n",
    "                           global_step=t)\n",
    "\n",
    "    elif para.classification == 5:\n",
    "\n",
    "        writer.add_scalars(main_tag=para.info_str+'_evaluation/cv',\n",
    "                           tag_scalar_dict={\n",
    "                               'accuracy': accuracy_cv,\n",
    "                               'precision0': precision_cv[0],\n",
    "                               'precision1': precision_cv[1],\n",
    "                               'precision2': precision_cv[2],\n",
    "                               'precision3': precision_cv[3],\n",
    "                               'precision4': precision_cv[4]},\n",
    "                           global_step=t)\n",
    "\n",
    "        writer.add_scalars(main_tag=para.info_str+'_evaluation/train',\n",
    "                           tag_scalar_dict={\n",
    "                               'accuracy': accuracy_train,\n",
    "                               'precision0': precision_train[0],\n",
    "                               'precision1': precision_train[1],\n",
    "                               'precision2': precision_train[2],\n",
    "                               'precision3': precision_train[3],\n",
    "                               'precision4': precision_train[4]},\n",
    "                           global_step=t)\n",
    "\n",
    "    writer.add_scalars(main_tag=para.info_str+'_loss/cv',\n",
    "                       tag_scalar_dict={\n",
    "                           'loss': loss_cv},\n",
    "                       global_step=t)\n",
    "\n",
    "    writer.add_scalars(main_tag=para.info_str+'_loss/train',\n",
    "                       tag_scalar_dict={\n",
    "                           'loss': loss_train},\n",
    "                       global_step=t)\n",
    "    writer.flush()\n",
    "\n",
    "    time_end = time.time()\n",
    "    print('Time cost = %fs' % (time_end - time_start))\n",
    "    print()\n",
    "\n",
    "writer.close()\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 保存模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish save model!\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), para.save_model_path)\n",
    "\n",
    "print('Finish save model!')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # captum\n",
    "# from captum.attr import IntegratedGradients\n",
    "#\n",
    "# ig = IntegratedGradients(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# temp = cv_dataloader.dataset.tensors[0]\n",
    "# temp.requires_grad_()\n",
    "# attr, delta = ig.attribute(temp,target=1, return_convergence_delta=True)\n",
    "# attr = attr.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Helper method to print importances and visualize distribution\n",
    "# def visualize_importances(feature_names, importances, title=\"Average Feature Importances\", plot=True, axis_title=\"Features\"):\n",
    "#     print(title)\n",
    "#     for i in range(len(feature_names)):\n",
    "#         print(feature_names[i], \": \", '%.3f'%(importances[i]))\n",
    "#     y_pos = (np.arange(len(feature_names)))\n",
    "#     if plot:\n",
    "#         plt.figure(figsize=(20,6))\n",
    "#         plt.barh(y_pos, importances, align='center')\n",
    "#         plt.yticks(y_pos, feature_names)\n",
    "#         plt.ylabel(axis_title)\n",
    "#         plt.grid(axis='y')\n",
    "#         plt.title(title)\n",
    "# visualize_importances(feature_names=X_cv.columns.values.tolist(), importances=np.mean(attr, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# X_cv.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# loss = nn.CrossEntropyLoss()\n",
    "# input = torch.Tensor(\n",
    "#     [[-0.0441,  0.0773],\n",
    "#     [-0.0781, -0.1772],\n",
    "#     [-0.1319, -0.0432],\n",
    "#     [-0.0714, -0.1261],\n",
    "#     [-0.0806, -0.1370],\n",
    "#     [-0.1730, -0.1472],\n",
    "#     [-0.0350, -0.0507],\n",
    "#     [-0.1149, -0.2248]])\n",
    "# # input = input.reshape(-1,4)\n",
    "# target = torch.Tensor([0, 1, 1, 0, 0, 0, 0, 0]).type(torch.LongTensor)\n",
    "# print(input.dtype)\n",
    "# print(target.dtype)\n",
    "# output = loss(input, target)\n",
    "# print(input, target, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Example of target with class indices\n",
    "# loss = nn.CrossEntropyLoss()\n",
    "# input = torch.randn(3, 5, requires_grad=True)\n",
    "# target = torch.Tensor([1,4,1]).type(torch.LongTensor)\n",
    "# output = loss(input, target)\n",
    "# print(input,target,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# loss = nn.BCEWithLogitsLoss()\n",
    "# input = torch.Tensor([0.5, 0.4, 0.3])\n",
    "# target = torch.Tensor([0])\n",
    "# output = loss(input, target)\n",
    "# print(input, target, output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}